# -*- coding: utf-8 -*-
"""myModel3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k6IfpNG23dnt2oOVpVX6aYnK3F1LGUie
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import TextVectorization
from tensorflow.keras.layers import Dropout
from tensorflow.keras.callbacks import EarlyStopping

import joblib

#from tensorflow.keras import layers

df = pd.read_csv(r"C:\project\myModel\mycode\model\datasetAID3_updated.csv")
#df.head()

label_map = {'Drowning': 0, 'Strangulation': 1, 'Burns': 2}
df['label'] = df['label'].map(label_map)

texts = df['text'].astype(str).tolist()
labels = df['label'].tolist()

#df.head()

# """Create the text encoder
# כאשר מגדירים את הפרמטר output_sequence_length בשכבת ה־TextVectorization, היא מבצעת באופן אוטומטי מילוי (padding) או קיטום (truncation) של הרצפים כדי להתאים לאורך שנקבע. המילוי מתבצע על ידי הוספת אפסים (0) בסוף הרצפים הקצרים יותר. לכן, גם אם לא ביצעתי מילוי מפורש באמצעות פונקציות כמו pad_sequences, הרצפים  כבר כוללים אפסים המייצגים מילוי.​
# """

texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, random_state=42)
VOCAB_SIZE = 1000
SEQUENCE_LENGTH = 100
# הגדרת שכבת המרה
encoder = TextVectorization(
    max_tokens=VOCAB_SIZE,
    output_mode='int',
    output_sequence_length=SEQUENCE_LENGTH )
encoder.adapt(texts_train)

# vocab = np.array(encoder.get_vocabulary())
# vocab[:20]

train_data = encoder(np.array(texts_train))
test_data = encoder(np.array(texts_test))

train_labels = np.array(labels_train)
test_labels = np.array(labels_test)

#
# sample_output = encoder(tf.constant(["example to text"]))
# print(sample_output)

"""2. שכבת Embedding
שכבת ה־Embedding ממירה אינדקסים לווקטורים צפופים. הפרמטרים של :​

input_dim=VOCAB_SIZE: גודל המילון (מספר המילים).

output_dim=64: גודל הווקטור לכל מילה.

mask_zero=True: מתעלמת מהאינדקס 0 (המייצג מילוי) במהלך העיבוד.​
"""

model = tf.keras.Sequential([
    #encoder,
    tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, output_dim=64, mask_zero=True),
    Dropout(0.3),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,dropout=0.2,recurrent_dropout=0.2)),
    Dropout(0.3),
    tf.keras.layers.Dense(64, activation='relu'),
    Dropout(0.4),
    tf.keras.layers.Dense(3, activation='softmax')
])

model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)
early_stop = EarlyStopping(
    monitor='val_loss',        # או 'val_accuracy'
    patience=3,                # מספר אפוקים בלי שיפור
    restore_best_weights=True  # משחזר בסיום את המשקלים עם התוצאה הטובה ביותר
)

# model.build(input_shape=(None, SEQUENCE_LENGTH))
# model.summary()

# train_data_np = train_data.numpy()
# test_data_np  = test_data.numpy()
# # אימון המודל
# history = model.fit(train_data_np,
#                     labels_train,
#                     epochs=10,
#                     batch_size=32,
#                     validation_data=(test_data_np, labels_test))

# המרת ה‑EagerTensor ל‑NumPy array
train_data_np = train_data.numpy()
test_data_np  = test_data.numpy()

history = model.fit(
     train_data_np,      # numpy במקום tf.Tensor
     train_labels,       # כבר numpy
     epochs=10,
     batch_size=32,
     validation_data=(test_data_np, test_labels),
    callbacks=[early_stop]
 )

labels_test_np = np.array(labels_test)

loss, accuracy = model.evaluate(test_data_np, labels_test_np)
print(f"Loss: {loss:.4f}, Accuracy: {accuracy:.4f}")

# new_texts = ["I was burned by boiling oil", "My friend swallowed a lot of water and is gasping for breath."]
# new_data = encoder(np.array(new_texts))
# predictions = model.predict(new_data)
# predicted_classes = np.argmax(predictions, axis=1)
#
# for text, pred_class in zip(new_texts, predicted_classes):
#     print(f"Text: {text} -> Predicted Class: {pred_class}")
joblib.dump(encoder, r"C:\project\myModel\mycode\model\encoder.pkl")

model.save(r"C:\project\myModel\mycode\model\saved_model.keras")
